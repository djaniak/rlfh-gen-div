{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resp_set = [\"bla bla\", \"daj wiersz\", \"zaszczekaj glosno i dlugo\"]\n",
    "\n",
    "from rlvsil.diversity.diversity_metrics import (\n",
    "    CosineSimilarity2Diversity,\n",
    "    AveragedCosineSimilarity,\n",
    "    AveragedDistinctNgrams,\n",
    "    DistinctNgrams,\n",
    ")\n",
    "\n",
    "\n",
    "def print_metric(metric, resp_set):\n",
    "    print(\"{0}: {1:0.3f}\".format(type(metric).__name__, metric(resp_set)))\n",
    "\n",
    "\n",
    "# TEST\n",
    "resp_set = [\"i am going\", \"i am going\", \"lets go i i\"]\n",
    "config = {\"n\": 3}\n",
    "print_metric(CosineSimilarity2Diversity(config), resp_set)\n",
    "print_metric(DistinctNgrams(config), resp_set)\n",
    "\n",
    "avg_config = {\"n_min\": 1, \"n_max\": 5}\n",
    "print_metric(AveragedCosineSimilarity(avg_config), resp_set)\n",
    "print_metric(AveragedDistinctNgrams(avg_config), resp_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We refer to expectation-adjusted distinct N-grams, sentence-BERT average cosine similarity and NLI diversity as EAD, Sent BERT and NLI respectively. \n",
    "\n",
    "We can view them as measuring syntactic, semantic and logical diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlvsil.diversity import DEFAULT_CONFIGS, calculate_diversity_metrics\n",
    "\n",
    "\n",
    "metrics = [\"ead_averaged_distinct_ngrams\", \"nli_sample_from_sim\", \"sent_bert_from_sim\"]\n",
    "\n",
    "outputss = [\n",
    "    [\n",
    "        \"I like to eat apples.\",\n",
    "        \"I like to eat bananas.\",\n",
    "        \"I like to eat oranges.\",\n",
    "    ],\n",
    "    [\n",
    "        \"I love to eat apples.\",\n",
    "        \"I love to eat bananas.\",\n",
    "        \"I love to eat oranges.\",\n",
    "    ],\n",
    "    [\n",
    "        \"I love muching on apples.\",\n",
    "        \"I love muching on bananas.\",\n",
    "        \"I love muching on oranges.\",\n",
    "    ],\n",
    "]\n",
    "\n",
    "config = DEFAULT_CONFIGS.copy()\n",
    "config = {k: v for k, v in config.items() if k in metrics}\n",
    "config[\"sample_overall\"] = True\n",
    "config[\"no_overall_input\"] = True\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = calculate_diversity_metrics(outputss, metric_configs=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint({k: round(v, 3) for k, v in results.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers==4.36.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"clarin-knext/summarization-chat-annotated\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_text = dataset[\"train\"][0][\"doc_text\"]\n",
    "\n",
    "prompt = f\"Twoim zadaniem jest przeczytanie podanego tekstu i napisanie streszczenia w języku polskim. Streszczenie powinno zawierać najważniejsze informacje i wydarzenia opisane w tekście, być zwięzłe i dobrze zorganizowane. Unikaj wprowadzania nowych informacji oraz osobistych opinii.\\n\\n###\\n\\nTekst: {doc_text}\\n\\nStreszczenie:\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "# model_name = \"speakleash/Bielik-7B-v0.1\"\n",
    "model_name = \"speakleash/Bielik-7B-Instruct-v0.1\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, torch_dtype=torch.float16, load_in_8bit=True, device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "outputs = []\n",
    "\n",
    "\n",
    "prompt_input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "summarization_ratio_lenth = 0.15\n",
    "max_length = int(len(prompt_input_ids[0]) * (1 + summarization_ratio_lenth))\n",
    "print(max_length)\n",
    "\n",
    "for _ in tqdm(range(16)):\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            prompt_input_ids.to(device),\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            max_length=max_length,\n",
    "            temperature=1.0,\n",
    "            top_k=0,\n",
    "            top_p=1,\n",
    "        )\n",
    "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(generated_text)\n",
    "    print(\"==============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read json\n",
    "import json\n",
    "\n",
    "with open(\"../Bielik-7B-v0.1_summarization-chat-annotated.json\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "info",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
