{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CosineSimilarity2Diversity: 0.667\n",
      "DistinctNgrams: 0.750\n",
      "AveragedCosineSimilarity: 0.737\n",
      "AveragedDistinctNgrams: 0.593\n"
     ]
    }
   ],
   "source": [
    "# resp_set = [\"bla bla\", \"daj wiersz\", \"zaszczekaj glosno i dlugo\"]\n",
    "\n",
    "from rlvsil.diversity.diversity_metrics import CosineSimilarity2Diversity, AveragedCosineSimilarity, AveragedDistinctNgrams, DistinctNgrams\n",
    "\n",
    "\n",
    "def print_metric(metric, resp_set):\n",
    "    print(\"{0}: {1:0.3f}\".format(type(metric).__name__, metric(resp_set)))\n",
    "\n",
    "# TEST\n",
    "resp_set = [\"i am going\", \"i am going\", \"lets go i i\"]\n",
    "config = {\"n\": 3}\n",
    "print_metric(CosineSimilarity2Diversity(config), resp_set)\n",
    "print_metric(DistinctNgrams(config), resp_set)\n",
    "\n",
    "avg_config = {\"n_min\": 1, \"n_max\": 5}\n",
    "print_metric(AveragedCosineSimilarity(avg_config), resp_set)\n",
    "print_metric(AveragedDistinctNgrams(avg_config), resp_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We refer to expectation-adjusted distinct N-grams, sentence-BERT average cosine similarity and NLI diversity as EAD, Sent BERT and NLI respectively. \n",
    "\n",
    "We can view them as measuring syntactic, semantic and logical diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ead_averaged_distinct_ngrams': {'n_max': 5, 'n_min': 1, 'vocab_size': 50257},\n",
      " 'nli_sample_from_sim': {'model_name': 'roberta-large-mnli',\n",
      "                         'n': 5,\n",
      "                         'top_k': 1},\n",
      " 'no_overall_input': True,\n",
      " 'sample_overall': True,\n",
      " 'sent_bert_from_sim': {}}\n"
     ]
    }
   ],
   "source": [
    "from rlvsil.diversity import DEFAULT_CONFIGS, calculate_diversity_metrics\n",
    "\n",
    "\n",
    "metrics = ['ead_averaged_distinct_ngrams' , 'nli_sample_from_sim' , 'sent_bert_from_sim']\n",
    "\n",
    "outputss = [\n",
    "    [\n",
    "        \"I like to eat apples.\",\n",
    "        \"I like to eat bananas.\",\n",
    "        \"I like to eat oranges.\",\n",
    "    ],\n",
    "    [\n",
    "        \"I love to eat apples.\",\n",
    "        \"I love to eat bananas.\",\n",
    "        \"I love to eat oranges.\",\n",
    "    ],\n",
    "    [\n",
    "        \"I love muching on apples.\",\n",
    "        \"I love muching on bananas.\",\n",
    "        \"I love muching on oranges.\",\n",
    "    ],\n",
    "]\n",
    "\n",
    "config = DEFAULT_CONFIGS.copy()\n",
    "config = {k:v for k,v in config.items() if k in metrics}\n",
    "config[\"sample_overall\"] = True\n",
    "config[\"no_overall_input\"] = True\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djaniak/mambaforge/envs/rlhf-gen-div-new/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating per-input diversities\n",
      "Average per-input diversities:\n",
      "{'mean_per_input_ead_averaged_distinct_ngrams': 0.6378211785884894, 'mean_per_input_sent_bert_from_sim': 0.40668171644210815, 'mean_per_input_nli_sample_from_sim': 0.9585304107930925}\n",
      "Std per-input diversities:\n",
      "{'std_per_input_ead_averaged_distinct_ngrams': 0.0, 'std_per_input_sent_bert_from_sim': 0.0597790032252103, 'std_per_input_nli_sample_from_sim': 0.045166222806895744}\n",
      "calculating overall diversities\n",
      "Average overall diversities:\n",
      "{'overall_ead_averaged_distinct_ngrams': 0.538994964957481, 'overall_sent_bert_from_sim': 0.3989078998565674, 'overall_nli_sample_from_sim': 0.9326768695645862}\n",
      "calculating overall single-input diversities\n",
      "Average overall single-input diversities:\n",
      "{'overall_single_output_ead_averaged_distinct_ngrams': 0.8345037971855376, 'overall_single_output_sent_bert_from_sim': 0.09744042158126831, 'overall_single_output_nli_sample_from_sim': 0.9376957925160726}\n"
     ]
    }
   ],
   "source": [
    "results = calculate_diversity_metrics(outputss, metric_configs=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_per_input_ead_averaged_distinct_ngrams': 0.638,\n",
      " 'mean_per_input_nli_sample_from_sim': 0.959,\n",
      " 'mean_per_input_sent_bert_from_sim': 0.407,\n",
      " 'overall_ead_averaged_distinct_ngrams': 0.539,\n",
      " 'overall_nli_sample_from_sim': 0.933,\n",
      " 'overall_sent_bert_from_sim': 0.399,\n",
      " 'overall_single_output_ead_averaged_distinct_ngrams': 0.835,\n",
      " 'overall_single_output_nli_sample_from_sim': 0.938,\n",
      " 'overall_single_output_sent_bert_from_sim': 0.097,\n",
      " 'std_per_input_ead_averaged_distinct_ngrams': 0.0,\n",
      " 'std_per_input_nli_sample_from_sim': 0.045,\n",
      " 'std_per_input_sent_bert_from_sim': 0.06}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint({k: round(v, 3) for k,v in results.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers==4.36.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d5ef417989f4f84827d13a8537f47c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/257 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef3469a1a2c41d3adca834bf09313b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/201M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d48087d8ee4a11aaade3a7a28553c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('clarin-knext/summarization-chat-annotated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'doc_text', 'summary'],\n",
       "        num_rows: 32480\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'complexqa_806054',\n",
       " 'doc_text': 'Zamach w Sarajewie Zamach w Sarajewie – zamach na następcę austro-węgierskiego tronu, arcyksięcia Franciszka Ferdynanda i jego żonę Zofię, księżnę Hohenberg, dokonany 28 czerwca 1914 roku przez bośniackiego Serba Gavrila Principa, członka serbskiej nacjonalistycznej organizacji \"Młoda Bośnia\". Princip wraz z pięcioma innymi uczestnikami zamachu powiązany był z serbską tajną organizacją \"Zjednoczenie lub śmierć\", popularnie nazywaną \"Czarna Ręka\", którą kierował Dragutin Dimitrijević pseudonim \"Apis\", szef serbskiego wywiadu wojskowego. Morderstwo następcy tronu austro-węgierskiego stworzyło napięcie pomiędzy Wiedniem a Belgradem. Austro-Węgry wysunęły ultimatum domagając się usunięcia wrogiej propagandy, a także udziału reprezentantów rządu cesarsko-królewskiego w śledztwie na terenie Serbii. Odrzucenie części żądań przez Serbię doprowadziło do tzw. kryzysu lipcowego, a w konsekwencji do wybuchu I wojny światowej. Przygotowania Przygotowania do zamachu rozpoczęto w marcu 1914 r., gdy prasa zapowiedziała przyjazd Franciszka Ferdynanda do Sarajewa. Ćwiczenia prowadzono w lasach pod Belgradem. Nedeljko Čabrinović, Gavrilo Princip i Trifko Grabež dotarli do Sarajewa 4 czerwca i zamieszkali u krewnych lub znajomych. Zamachowcy byli wyposażeni w cztery pistolety, sześć bomb domowej roboty i kapsułki z cyjankiem. Koordynacją działań na miejscu zajmował się miejscowy dziennikarz Danilo Ilić, on też zwerbował na miejscu Cvjetka Popovicia i Muhameda Mehmedbašicia. Zamachowcy zajęli pozycje na liczącej 4 km trasie z dworca kolejowego do ratusza, gdzie na arcyksięcia czekały władze miasta. Obstawili odcinek 300 metrów od Mostu Ćumurija, gdzie czekali Popović i Ilić, do Mostu Cesarskiego. Przy kawiarni Mostar oczekiwali Mehmedbašić i Čubrilović, Čabrinović na wysokości Drewnianego Mostu, Princip przy Moście Łacińskim, a Grabež przy Moście Cesarskim. Przebieg wydarzenia 28 czerwca 1914 roku arcyksiążę Franciszek Ferdynand Habsburg wraz z towarzyszącą mu małżonką Zofią von Chotek obchodzili rocznicę swojego ślubu. Z tej okazji wzięli udział we mszy odprawionej w hotelu „Bosna” w kurorcie Ilidža (godz. 9:00), 10 km na zachód od Sarajewa, do którego wyruszyli pociągiem około godz. 9:42. Do samego Sarajewa para przybyła o godz. 10:07. Na dworcu miejskim czekała kolumna sześciu aut, w tym oliwkowoszara sportowa limuzyna marki Gräf &amp; Stift ze składanym dachem dla wizytującej pary. W limuzynie wraz z Franciszkiem Ferdynandem i jego żoną jechali także: szofer Leopold Lojka, szef straży przybocznej – hrabia Franz von Harrach oraz wojskowy gubernator Bośni i Hercegowiny – Oskar Potiorek. Trasa przejazdu była publicznie podana do wiadomości kilka dni wcześniej. W trakcie przejazdu pierwszemu z zamachowców, Mehmedbašiciowi, nie udało się rozeznać, w którym aucie siedzi arcyksiążę. Kolejny ze spiskowców, Vaso Čubrilović, nie odważył się strzelać, widząc w pojeździe Franciszka i jego żonę, a Popović stał zbyt daleko i ze względu na słaby wzrok nie wiedział, w kogo celować. Pierwsza próba zamachu miała miejsce około godz. 10:26 przy Moście Ćumurija. Čabrinović rzucił w kierunku auta ręczny granat, ale dzięki refleksowi kierowcy limuzyny z arcyksięciem granat odbił się od płóciennego dachu samochodu i eksplodując, zniszczył następny samochód jadący w kolumnie raniąc pasażerów, policjanta i osoby z tłumu – łącznie 17 osób. Stojący w pobliżu Princip nie zaatakował, gdyż jego uwagę przyciągnęło aresztowanie Čabrinovicia, a Grabež był nieprzygotowany na to, że samochód arcyksięcia będzie poruszać się tak szybko. Para książęca przybyła do sarajewskiego ratusza o godzinie 10:30. Dziesięć minut później Franciszek Ferdynand z żoną opuścił ratusz z zamiarem odwiedzenia rannych w wyniku eksplozji, a ochrona zdecydowała o zmianie trasy przejazdu. Samochód z arcyksięciem, zwalniając na zakręcie na wysokości Mostu Łacińskiego, mijał kolejnego spiskowca, Grabeža, który nie chciał ryzykować postrzału postronnych osób. W tym miejscu samochód miał skręcić w prawo, a nie jak pierwotnie planowano pojechać prosto, ale kierowca nie dowiedział się o zmianie planów. Po przejechaniu skrzyżowania szofer zatrzymał auto na rozkaz szefa ochrony i zaczął zawracać. Drugi ze znajdujących się przypadkowo przy drugim przejeździe arcyksięcia zamachowiec, Princip, oddał dwa strzały w kierunku pary arcyksiążęcej. Pierwszy strzał trafił Zofię von Chotek w podbrzusze, druga kula przeszyła tchawicę i tętnicę szyjną jej męża. Oboje nie przeżyli (atak Principa około godz. 10:50). Napastnika zauważył detektyw Smail Spahović, który usiłował go obezwładnić, jednak Princip uderzył go pistoletem w głowę. Princip usiłował się zastrzelić, ale broń wytrącił mu detektyw Anto Velić. Następnie bezskutecznie próbował otruć się przygotowaną dawką cyjanku. Tłum chciał go zlinczować, lecz nie dopuściła do tego policja. Kolumna samochodów ze śmiertelnie postrzeloną parą o godz. 10:55 dotarła do rezydencji (konaku), gdzie urzędował Oskar Potiorek, współpasażer tragicznej jazdy pary arcyksiążęcej. Akt zgonu określił śmierć arcyksięcia Ferdynanda i jego żony na',\n",
       " 'summary': '28 czerwca 1914 roku w Sarajewie doszło do zamachu na arcyksięcia Franciszka Ferdynanda i jego żonę Zofię. Zamach został przeprowadzony przez bośniackiego Serba Gavrila Principa, członka serbskiej organizacji \"Młoda Bośnia\". Princip był powiązany z tajną organizacją \"Czarna Ręka\", która kierowała serbskim wywiadem wojskowym. Zamach wywołał napięcie między Austro-Węgrami a Serbią, co doprowadziło do wybuchu I wojny światowej. Przygotowania do zamachu rozpoczęto w marcu 1914 roku, a zamachowcy byli wyposażeni w broń i bomby domowej roboty. Zamach odbył się podczas przejazdu arcyksięcia i jego żony przez miasto. Pierwsza próba zamachu nie powiodła się, ale druga, przeprowadzona przez Principa, zakończyła się śmiercią arcyksięcia i jego żony. Zamachowiec został aresztowany i nie udało mu się popełnić samobójstwa. Zamach spowodował zmianę polityczną i wybuch wojny.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djaniak/mambaforge/envs/rlhf-gen-div-new/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfa6a78e70da48918e2edf4b9a29f462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "model_name = \"speakleash/Bielik-7B-v0.1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]/home/djaniak/mambaforge/envs/rlhf-gen-div-new/lib/python3.10/site-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 2102, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "  0%|          | 1/1000 [00:23<6:33:29, 23.63s/it]/home/djaniak/mambaforge/envs/rlhf-gen-div-new/lib/python3.10/site-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 419, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "  0%|          | 2/1000 [00:27<3:21:01, 12.09s/it]/home/djaniak/mambaforge/envs/rlhf-gen-div-new/lib/python3.10/site-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 2048, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "  0%|          | 2/1000 [00:39<5:29:53, 19.83s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/djaniak/projects/rlfh-gen-div/notebooks/diversity.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B156.17.248.228/home/djaniak/projects/rlfh-gen-div/notebooks/diversity.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m samples \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B156.17.248.228/home/djaniak/projects/rlfh-gen-div/notebooks/diversity.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(N):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B156.17.248.228/home/djaniak/projects/rlfh-gen-div/notebooks/diversity.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(input_ids\u001b[39m.\u001b[39;49mto(device),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B156.17.248.228/home/djaniak/projects/rlfh-gen-div/notebooks/diversity.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m                             pad_token_id\u001b[39m=\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B156.17.248.228/home/djaniak/projects/rlfh-gen-div/notebooks/diversity.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m                             do_sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B156.17.248.228/home/djaniak/projects/rlfh-gen-div/notebooks/diversity.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m                             max_length\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B156.17.248.228/home/djaniak/projects/rlfh-gen-div/notebooks/diversity.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m                             temperature\u001b[39m=\u001b[39;49mtemperature, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B156.17.248.228/home/djaniak/projects/rlfh-gen-div/notebooks/diversity.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m                             top_k\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B156.17.248.228/home/djaniak/projects/rlfh-gen-div/notebooks/diversity.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m                             top_p\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B156.17.248.228/home/djaniak/projects/rlfh-gen-div/notebooks/diversity.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m                             )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B156.17.248.228/home/djaniak/projects/rlfh-gen-div/notebooks/diversity.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     generated_text \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(output[\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B156.17.248.228/home/djaniak/projects/rlfh-gen-div/notebooks/diversity.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     samples\u001b[39m.\u001b[39mappend(generated_text)\n",
      "File \u001b[0;32m~/mambaforge/envs/rlhf-gen-div-new/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/mambaforge/envs/rlhf-gen-div-new/lib/python3.10/site-packages/transformers/generation/utils.py:1764\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1756\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1757\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1758\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1759\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1760\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1761\u001b[0m     )\n\u001b[1;32m   1763\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1764\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m   1765\u001b[0m         input_ids,\n\u001b[1;32m   1766\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1767\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mlogits_warper,\n\u001b[1;32m   1768\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1769\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1770\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1771\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1772\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1773\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1774\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1775\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1776\u001b[0m     )\n\u001b[1;32m   1778\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1779\u001b[0m     \u001b[39m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m     beam_scorer \u001b[39m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1781\u001b[0m         batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1782\u001b[0m         num_beams\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1787\u001b[0m         max_length\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mmax_length,\n\u001b[1;32m   1788\u001b[0m     )\n",
      "File \u001b[0;32m~/mambaforge/envs/rlhf-gen-div-new/lib/python3.10/site-packages/transformers/generation/utils.py:2861\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2858\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2860\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2861\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2862\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2863\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2864\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2865\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2866\u001b[0m )\n\u001b[1;32m   2868\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2869\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/rlhf-gen-div-new/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/rlhf-gen-div-new/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:1053\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1050\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1052\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1053\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   1054\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1055\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1056\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1057\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1058\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1059\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1060\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1061\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1062\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1063\u001b[0m )\n\u001b[1;32m   1065\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1066\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/mambaforge/envs/rlhf-gen-div-new/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/rlhf-gen-div-new/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:938\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    928\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    929\u001b[0m         decoder_layer\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    930\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    935\u001b[0m         use_cache,\n\u001b[1;32m    936\u001b[0m     )\n\u001b[1;32m    937\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 938\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    939\u001b[0m         hidden_states,\n\u001b[1;32m    940\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    941\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    942\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    943\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    944\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    945\u001b[0m     )\n\u001b[1;32m    947\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    949\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/mambaforge/envs/rlhf-gen-div-new/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/rlhf-gen-div-new/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:675\u001b[0m, in \u001b[0;36mMistralDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[39m# Fully Connected\u001b[39;00m\n\u001b[1;32m    674\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[0;32m--> 675\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpost_attention_layernorm(hidden_states)\n\u001b[1;32m    676\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(hidden_states)\n\u001b[1;32m    677\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/mambaforge/envs/rlhf-gen-div-new/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "N = 16\n",
    "temperature = 1.0  \n",
    "outputs = []\n",
    "\n",
    "for example in tqdm(dataset[\"train\"].select(range(1000))):\n",
    "    input_ids = tokenizer.encode(example[\"doc_text\"], return_tensors=\"pt\")\n",
    "    samples = []\n",
    "    for _ in range(N):\n",
    "        output = model.generate(input_ids.to(device),\n",
    "                                pad_token_id=tokenizer.eos_token_id,\n",
    "                                do_sample=True, \n",
    "                                max_length=50, \n",
    "                                temperature=temperature, \n",
    "                                top_k=0,\n",
    "                                top_p=1\n",
    "                                )\n",
    "        \n",
    "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        samples.append(generated_text)\n",
    "    outputs.append(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"speakleash/Bielik-Instruct-7B-v0.1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "info",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
